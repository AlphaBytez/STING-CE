#!/bin/bash
# STING LLM Service Manager
# Universal Ollama-based LLM management for all platforms

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
cd "$SCRIPT_DIR"

# Detect platform
PLATFORM="$(uname)"
SERVICE_NAME="llm-gateway"
PID_FILE="$HOME/.sting-ce/run/$SERVICE_NAME.pid"
LOG_FILE="$HOME/.sting-ce/logs/$SERVICE_NAME.log"
NATIVE_PORT="${NATIVE_LLM_PORT:-8086}"  # Use 8086 for native service to avoid conflicts
OLLAMA_PORT="${OLLAMA_PORT:-11434}"
EXTERNAL_AI_PORT="${EXTERNAL_AI_PORT:-8091}"

# Ollama configuration
OLLAMA_MODELS_TO_PRELOAD="${OLLAMA_MODELS:-deepseek-r1:latest}"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

log() {
    echo -e "${GREEN}[STING LLM]${NC} $1"
}

error() {
    echo -e "${RED}[ERROR]${NC} $1" >&2
}

warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

check_ollama_status() {
    log "Checking Ollama status..."
    if command -v ollama >/dev/null 2>&1; then
        if curl -sf http://localhost:$OLLAMA_PORT/api/tags >/dev/null 2>&1; then
            log "Ollama is running and accessible on port $OLLAMA_PORT"
            return 0
        else
            warning "Ollama is installed but not running"
            return 1
        fi
    else
        error "Ollama is not installed"
        return 1
    fi
}

start_ollama() {
    log "Starting Ollama service..."
    if command -v ollama >/dev/null 2>&1; then
        if ! curl -sf http://localhost:$OLLAMA_PORT/api/tags >/dev/null 2>&1; then
            log "Starting Ollama daemon..."
            ollama serve &
            sleep 5
            
            # Wait for Ollama to be ready
            for i in {1..30}; do
                if curl -sf http://localhost:$OLLAMA_PORT/api/tags >/dev/null 2>&1; then
                    log "Ollama is ready!"
                    return 0
                fi
                sleep 2
            done
            error "Ollama failed to start"
            return 1
        else
            log "Ollama is already running"
            return 0
        fi
    else
        error "Ollama is not installed. Please install Ollama first."
        return 1
    fi
}

check_external_ai_service() {
    log "Checking External AI service..."
    if curl -sf http://localhost:$EXTERNAL_AI_PORT/health >/dev/null 2>&1; then
        log "External AI service is running on port $EXTERNAL_AI_PORT"
        return 0
    else
        warning "External AI service is not running"
        return 1
    fi
}

start_external_ai_service() {
    log "Starting External AI service..."
    if [ -f "external_ai_service/app.py" ]; then
        # Ensure run directory exists
        mkdir -p "$HOME/.sting-ce/run"
        
        cd external_ai_service
        nohup python app.py > "$LOG_FILE.external-ai" 2>&1 &
        EXTERNAL_AI_PID=$!
        echo $EXTERNAL_AI_PID > "$HOME/.sting-ce/run/external-ai.pid"
        cd - > /dev/null
        
        # Wait for service to be ready
        for i in {1..30}; do
            if curl -sf http://localhost:$EXTERNAL_AI_PORT/health >/dev/null 2>&1; then
                log "External AI service is ready!"
                return 0
            fi
            sleep 2
        done
        error "External AI service failed to start"
        return 1
    else
        error "External AI service not found"
        return 1
    fi
}

check_status() {
    log "Checking STING LLM services status..."
    
    # Check Ollama (universal)
    check_ollama_status
    ollama_status=$?
    
    # Check External AI service
    check_external_ai_service
    external_ai_status=$?
    
    # Legacy service checks (for backward compatibility)
    legacy_status=1
    if [ "$PLATFORM" == "Darwin" ]; then
        # Check native service
        if [ -f "$PID_FILE" ]; then
            PID=$(cat "$PID_FILE")
            if kill -0 "$PID" 2>/dev/null; then
                log "Legacy native LLM service is running (PID: $PID)"
                legacy_status=0
            else
                warning "Legacy PID file exists but service is not running"
                rm -f "$PID_FILE"
            fi
        fi
    else
        # Check Docker service
        if docker ps | grep -q "sting-llm-gateway"; then
            log "Legacy Docker LLM service is running"
            legacy_status=0
        fi
    fi
    
    # Summary
    echo ""
    log "=== LLM Services Status ==="
    if [ $ollama_status -eq 0 ]; then
        log "✅ Ollama: Running"
    else
        log "❌ Ollama: Not running"
    fi
    
    if [ $external_ai_status -eq 0 ]; then
        log "✅ External AI Service: Running"
    else
        log "❌ External AI Service: Not running"
    fi
    
    if [ $legacy_status -eq 0 ]; then
        log "⚠️  Legacy LLM Service: Running (consider migrating to Ollama)"
    else
        log "ℹ️  Legacy LLM Service: Not running"
    fi
    
    # Return success if Ollama and External AI are running
    if [ $ollama_status -eq 0 ] && [ $external_ai_status -eq 0 ]; then
        log "✅ Modern LLM stack is operational"
        return 0
    else
        warning "Modern LLM stack needs attention"
        return 1
    fi
}

start_service() {
    log "Starting LLM services..."
    
    # Check if modern stack is already running
    if check_ollama_status >/dev/null 2>&1 && check_external_ai_service >/dev/null 2>&1; then
        log "Modern LLM stack is already running"
        return 0
    fi
    
    # Start Ollama first
    log "Starting Ollama service..."
    if ! start_ollama; then
        error "Failed to start Ollama"
        return 1
    fi
    
    # Start External AI service
    log "Starting External AI service..."
    if ! start_external_ai_service; then
        error "Failed to start External AI service"
        return 1
    fi
    
    log "✅ Modern LLM stack started successfully"
    return 0
    
    if [ "$PLATFORM" == "Darwin" ]; then
        # Start native service with MPS
        log "Starting native service with Metal Performance Shaders (MPS) support..."
        
        # Ensure directories exist
        mkdir -p "$(dirname "$PID_FILE")"
        mkdir -p "$(dirname "$LOG_FILE")"
        
        # Set up environment and activate virtual environment
        export PYTHONPATH="${PYTHONPATH}:$SCRIPT_DIR/llm_service"
        
        # Activate STING virtual environment if it exists
        VENV_PATH="$HOME/.sting-ce/.venv"
        if [ -d "$VENV_PATH" ] && [ -f "$VENV_PATH/bin/activate" ]; then
            log "Activating STING virtual environment..."
            source "$VENV_PATH/bin/activate"
            
            # Run dependency checker to ensure all required packages are installed
            if [ -f "$SCRIPT_DIR/scripts/check_llm_dependencies.sh" ]; then
                log "Verifying LLM dependencies..."
                INSTALL_DIR="$HOME/.sting-ce" bash "$SCRIPT_DIR/scripts/check_llm_dependencies.sh" || {
                    warning "Dependency check reported issues - attempting to continue"
                }
            fi
        else
            error "STING virtual environment not found. Please run install_sting.sh first."
            return 1
        fi
        
        # Use MODEL_PATH if provided, otherwise default based on config.yml and available models
        if [ -z "$MODEL_PATH" ]; then
            MODELS_DIR="${STING_MODELS_DIR:-$HOME/Downloads/llm_models}"
            # Default to phi3 for enterprise-grade Bee conversations (as specified in config.yml)
            export MODEL_NAME="phi3"  # Primary model for Bee interface
            export MODEL_PATH=""
            log "Using enterprise default model: $MODEL_NAME (HuggingFace)"
            
            # Alternative: Check for local models as backup if HuggingFace fails
            # Uncomment to enable local model fallback:
            # if [ -d "$MODELS_DIR/DeepSeek-R1-Distill-Qwen-1.5B" ]; then
            #     export MODEL_PATH="$MODELS_DIR/DeepSeek-R1-Distill-Qwen-1.5B"
            #     export MODEL_NAME="deepseek-1.5b"
            #     log "Fallback to local model: DeepSeek-R1-Distill-Qwen-1.5B"
            # elif [ -d "$MODELS_DIR/phi-2" ]; then
            #     export MODEL_PATH="$MODELS_DIR/phi-2"
            #     export MODEL_NAME="phi2"
            #     log "Fallback to local model: phi-2"
            # fi
        fi
        
        export DEVICE_TYPE="auto"
        export TORCH_DEVICE="auto"
        export PERFORMANCE_PROFILE="gpu_accelerated"
        export QUANTIZATION="none"
        export TORCH_PRECISION="fp16"  # Use fp16 for better memory efficiency
        export PORT="$NATIVE_PORT"
        export PYTORCH_ENABLE_MPS_FALLBACK=1
        export LLM_PRELOAD_ON_STARTUP="true"  # Preload default model on startup
        
        # Check Python
        if ! command -v python3 &> /dev/null; then
            error "Python 3 is required but not found"
            return 1
        fi
        
        # Kill any existing processes on the port
        if lsof -ti:$NATIVE_PORT >/dev/null 2>&1; then
            log "Killing existing processes on port $NATIVE_PORT..."
            lsof -ti:$NATIVE_PORT | xargs kill -9 2>/dev/null || true
            sleep 2
        fi
        
        # Start service
        cd llm_service
        nohup "$VENV_PATH/bin/python" server.py > "$LOG_FILE" 2>&1 &
        PID=$!
        echo $PID > "$PID_FILE"
        cd - > /dev/null
        
        log "Service started with PID: $PID"
        log "Logs: $LOG_FILE"
        
        # Wait for service to be ready
        log "Waiting for service to be ready..."
        for i in {1..30}; do
            if curl -sf http://localhost:$NATIVE_PORT/health >/dev/null 2>&1; then
                log "Service is ready!"
                
                # Auto-preload models if enabled
                if [ "${AUTO_PRELOAD_MODELS:-true}" == "true" ]; then
                    log "Auto-preloading models for immediate availability..."
                    sleep 2  # Give service a moment to stabilize
                    preload_models
                fi
                
                return 0
            fi
            sleep 2
        done
        
        warning "Service started but not responding to health checks"
        return 1
    else
        # Start Docker service
        log "Starting Docker LLM service..."
        cd "$SCRIPT_DIR"
        docker compose up -d llm-gateway
    fi
}

stop_service() {
    log "Stopping LLM service..."
    
    if [ "$PLATFORM" == "Darwin" ]; then
        # Stop native service
        if [ -f "$PID_FILE" ]; then
            PID=$(cat "$PID_FILE")
            if kill -0 "$PID" 2>/dev/null; then
                log "Stopping native service (PID: $PID)..."
                kill "$PID"
                sleep 2
                if kill -0 "$PID" 2>/dev/null; then
                    warning "Service didn't stop gracefully, forcing..."
                    kill -9 "$PID"
                fi
            fi
            rm -f "$PID_FILE"
            log "Service stopped"
        else
            log "Service is not running"
        fi
    else
        # Stop Docker service
        log "Stopping Docker service..."
        docker compose stop llm-gateway
    fi
}

restart_service() {
    stop_service
    sleep 2
    start_service
}

show_logs() {
    if [ "$PLATFORM" == "Darwin" ]; then
        if [ -f "$LOG_FILE" ]; then
            tail -f "$LOG_FILE"
        else
            error "No log file found at $LOG_FILE"
        fi
    else
        docker logs -f sting-llm-gateway-1
    fi
}

list_models() {
    log "Checking available models..."
    
    # Get service URL
    if [ "$PLATFORM" == "Darwin" ]; then
        SERVICE_URL="http://localhost:$NATIVE_PORT"
    else
        SERVICE_URL="http://localhost:8085"
    fi
    
    # Check if service is running
    if ! check_status >/dev/null 2>&1; then
        error "LLM service is not running. Start it first with: $0 start"
        return 1
    fi
    
    # Get models status
    response=$(curl -sf "$SERVICE_URL/models" 2>/dev/null)
    if [ $? -eq 0 ]; then
        echo -e "\n${GREEN}Available Models:${NC}"
        echo "$response" | python3 -m json.tool | grep -A 100 '"available_models"' | grep -E '^\s+"' | sed 's/[",]//g' | sed 's/^[[:space:]]*/  - /'
        
        echo -e "\n${GREEN}Loaded Models:${NC}"
        loaded=$(echo "$response" | python3 -c "import json, sys; data=json.load(sys.stdin); loaded=data.get('loaded_models', {}); print('\n'.join([f'  - {m}: last used {info["last_used"]}, priority {info["priority"]}' for m, info in loaded.items()]) if loaded else '  None')" 2>/dev/null)
        if [ -z "$loaded" ]; then
            echo "  None"
        else
            echo "$loaded"
        fi
        
        echo -e "\n${GREEN}Configuration:${NC}"
        echo "$response" | python3 -c "import json, sys; data=json.load(sys.stdin); print(f'  Max loaded models: {data.get("max_loaded_models", "N/A")}'); print(f'  Idle timeout: {data.get("idle_timeout_minutes", "N/A")} minutes'); print(f'  Development mode: {data.get("development_mode", False)}')" 2>/dev/null
    else
        error "Failed to get models status"
        return 1
    fi
}

load_model() {
    local model_name="$1"
    if [ -z "$model_name" ]; then
        error "Model name is required"
        echo "Usage: $0 load <model_name>"
        echo "Available models: tinyllama, phi2, llama3, deepseek-1.5b, dialogpt"
        return 1
    fi
    
    log "Loading model: $model_name"
    
    # Get service URL
    if [ "$PLATFORM" == "Darwin" ]; then
        SERVICE_URL="http://localhost:$NATIVE_PORT"
    else
        SERVICE_URL="http://localhost:8085"
    fi
    
    # Check if service is running
    if ! check_status >/dev/null 2>&1; then
        error "LLM service is not running. Start it first with: $0 start"
        return 1
    fi
    
    # Use dedicated load endpoint
    log "Sending load request..."
    response=$(curl -sf -X POST "$SERVICE_URL/models/$model_name/load" 2>/dev/null)
    
    if [ $? -eq 0 ]; then
        load_time=$(echo "$response" | python3 -c "import json, sys; print(json.load(sys.stdin).get('load_time_seconds', 'N/A'))" 2>/dev/null)
        log "Model $model_name loaded successfully in ${load_time}s!"
        # Show current models status
        list_models
    else
        error "Failed to load model $model_name"
        return 1
    fi
}

unload_model() {
    local model_name="$1"
    if [ -z "$model_name" ]; then
        error "Model name is required"
        echo "Usage: $0 unload <model_name>"
        return 1
    fi
    
    log "Unloading model: $model_name"
    
    # Get service URL
    if [ "$PLATFORM" == "Darwin" ]; then
        SERVICE_URL="http://localhost:$NATIVE_PORT"
    else
        SERVICE_URL="http://localhost:8085"
    fi
    
    # Check if service is running
    if ! check_status >/dev/null 2>&1; then
        error "LLM service is not running. Start it first with: $0 start"
        return 1
    fi
    
    # Send unload request
    response=$(curl -sf -X POST "$SERVICE_URL/models/$model_name/unload" 2>/dev/null)
    
    if [ $? -eq 0 ]; then
        log "Model $model_name unloaded successfully!"
        # Show current models status
        list_models
    else
        error "Failed to unload model $model_name"
        return 1
    fi
}

preload_models() {
    log "Preloading configured models..."
    
    # Get service URL
    if [ "$PLATFORM" == "Darwin" ]; then
        SERVICE_URL="http://localhost:$NATIVE_PORT"
    else
        SERVICE_URL="http://localhost:8085"
    fi
    
    # Check if service is running
    if ! check_status >/dev/null 2>&1; then
        error "LLM service is not running. Start it first with: $0 start"
        return 1
    fi
    
    # Default models to preload based on priority (enterprise-grade models)
    MODELS_TO_PRELOAD="${PRELOAD_MODELS:-phi3}"
    
    # Convert space-separated list to JSON array
    if [ -n "$MODELS_TO_PRELOAD" ]; then
        models_json=$(echo "$MODELS_TO_PRELOAD" | python3 -c "import sys, json; print(json.dumps(sys.stdin.read().strip().split()))")
        request_body="{"models": $models_json}"
    else
        request_body="{}"
    fi
    
    log "Preloading models: $MODELS_TO_PRELOAD"
    
    # Use dedicated preload endpoint
    response=$(curl -sf -X POST "$SERVICE_URL/models/preload" \
        -H "Content-Type: application/json" \
        -d "$request_body" \
        2>/dev/null)
    
    if [ $? -eq 0 ]; then
        # Parse and display results
        echo "$response" | python3 -c "
import json, sys
data = json.load(sys.stdin)
for model, result in data.get('preloaded', {}).items():
    if result['status'] == 'success':
        print(f'✓ {model} loaded in {result["load_time_seconds"]}s')
    else:
        print(f'✗ {model} failed: {result.get("message", "Unknown error")}')
" 2>/dev/null
        
        log "Preloading complete!"
        list_models
    else
        error "Failed to preload models"
        return 1
    fi
}

test_routing() {
    local message="${1:-Hello, how are you?}"
    
    log "Testing task routing..."
    
    # Get service URL
    if [ "$PLATFORM" == "Darwin" ]; then
        SERVICE_URL="http://localhost:$NATIVE_PORT"
    else
        SERVICE_URL="http://localhost:8085"
    fi
    
    # Check if service is running
    if ! check_status >/dev/null 2>&1; then
        error "LLM service is not running. Start it first with: $0 start"
        return 1
    fi
    
    # Test routing decision
    response=$(curl -sf -X POST "$SERVICE_URL/route" \
        -H "Content-Type: application/json" \
        -d "{"message": "$message"}" \
        2>/dev/null)
    
    if [ $? -eq 0 ]; then
        echo -e "\n${GREEN}Task Routing Analysis:${NC}"
        echo "Message: "$message""
        echo "$response" | python3 -m json.tool
    else
        error "Failed to analyze routing"
        return 1
    fi
}

# Main command handling
case "$1" in
    start)
        start_service
        ;;
    stop)
        stop_service
        ;;
    restart)
        restart_service
        ;;
    status)
        check_status
        ;;
    logs)
        show_logs
        ;;
    models|list)
        list_models
        ;;
    load)
        load_model "$2"
        ;;
    unload)
        unload_model "$2"
        ;;
    preload)
        preload_models
        ;;
    route|test-routing)
        test_routing "$2"
        ;;
    ollama-status)
        check_ollama_status
        ;;
    ollama-start)
        start_ollama
        ;;
    external-ai-status)
        check_external_ai_service
        ;;
    external-ai-start)
        start_external_ai_service
        ;;
    *)
        echo "STING LLM Service Manager (Universal Ollama Edition)"
        echo ""
        echo "🚀 Modern LLM Stack (Recommended):"
        echo "  $0 start              - Start Ollama + External AI services"
        echo "  $0 stop               - Stop all LLM services"
        echo "  $0 restart            - Restart all LLM services"
        echo "  $0 status             - Check all services status"
        echo ""
        echo "🔧 Individual Service Management:"
        echo "  $0 ollama-status      - Check Ollama status"
        echo "  $0 ollama-start       - Start Ollama service"
        echo "  $0 external-ai-status - Check External AI service"
        echo "  $0 external-ai-start  - Start External AI service"
        echo ""
        echo "📊 Model Management (Ollama):"
        echo "  $0 models             - List available Ollama models"
        echo "  $0 load <model>       - Load a specific Ollama model"
        echo "  $0 unload <model>     - Unload a specific Ollama model"
        echo ""
        echo "🧪 Testing:"
        echo "  $0 route [message]    - Test task routing"
        echo "  $0 logs               - Show service logs"
        echo ""
        echo "⚙️ Environment Variables:"
        echo "  OLLAMA_PORT           - Ollama port (default: 11434)"
        echo "  EXTERNAL_AI_PORT      - External AI port (default: 8091)"
        echo "  OLLAMA_MODELS         - Models to preload (default: 'deepseek-r1:latest')"
        echo ""
        echo "Platform: $PLATFORM"
        if [ "$PLATFORM" == "Darwin" ]; then
            echo "Mode: Native with MPS (GPU) acceleration"
        else
            echo "Mode: Docker container"
        fi
        exit 1
        ;;
esac