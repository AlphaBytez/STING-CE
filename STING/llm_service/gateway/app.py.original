from fastapi import FastAPI, HTTPException, Request
from pydantic import BaseModel
import httpx
import os
import json
import logging
from typing import Dict, Any, Optional, List
from ..filtering.toxicity import ToxicityFilter
from ..filtering.data_leakage import DataLeakageFilter
from .chat_api import router as chat_router

app = FastAPI(title="LLM Gateway Service")

# Include the chat router
app.include_router(chat_router)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("llm-gateway")

# Load configuration
with open("/app/conf/config.yml", "r") as f:
    import yaml
    config = yaml.safe_load(f).get('llm_service', {})

# Initialize filters
toxicity_filter = ToxicityFilter(
    enabled=config.get('filtering', {}).get('toxicity', {}).get('enabled', True),
    threshold=config.get('filtering', {}).get('toxicity', {}).get('threshold', 0.7)
)

data_leakage_filter = DataLeakageFilter(
    enabled=config.get('filtering', {}).get('data_leakage', {}).get('enabled', True),
    patterns=config.get('filtering', {}).get('data_leakage', {}).get('sensitive_patterns', [])
)

# Model endpoints
MODEL_ENDPOINTS = {
    model_name: model_config.get('endpoint')
    for model_name, model_config in config.get('models', {}).items()
}

DEFAULT_MODEL = config.get('default_model', 'llama3')

class QueryRequest(BaseModel):
    message: str
    model: Optional[str] = None
    max_tokens: Optional[int] = None
    temperature: Optional[float] = 0.7

class QueryResponse(BaseModel):
    response: str
    model_used: str
    filtered: bool = False
    filter_reason: Optional[str] = None

@app.post("/generate", response_model=QueryResponse)
async def generate_response(request: QueryRequest):
    # Choose model
    model = request.model or DEFAULT_MODEL
    
    if model not in MODEL_ENDPOINTS:
        raise HTTPException(status_code=400, detail=f"Model {model} not supported")
    
    # Try to use the actual model service first
    try:
        # Call the model service
        async with httpx.AsyncClient(timeout=30.0) as client:
            logger.info(f"Attempting to call model service for {model} at {MODEL_ENDPOINTS[model]}")
            
            response = await client.post(
                MODEL_ENDPOINTS[model],
                json={
                    "message": request.message,
                    "max_tokens": request.max_tokens or config.get('models', {}).get(model, {}).get('max_tokens', 1024),
                    "temperature": request.temperature
                }
            )
            
            if response.status_code != 200:
                logger.warning(f"Model service error: {response.text} - falling back to development mode")
                raise HTTPException(
                    status_code=response.status_code,
                    detail=f"Model service error: {response.text}"
                )
            
            llm_response = response.json().get("response")
            logger.info(f"Successfully received response from model service")
            
    except (httpx.RequestError, HTTPException) as e:
        # Fallback to development mode if the call fails
        logger.info(f"Model service unavailable, using development mode for {model}: {str(e)}")
        
        # Generate a mock response based on the query
        mock_responses = [
            f"This is a development mock response from the {model} model. In production, this would call the actual LLM API.",
            f"Development mode response: The LLM Gateway is functioning, but no actual model is loaded for {model}.",
            f"Mock response from {model}: When properly configured with downloaded models, this would provide a real AI response.",
            f"Development testing response from {model}: Your message was processed, but actual models are not available.",
        ]
        import random
        llm_response = random.choice(mock_responses) + f"\n\nYour message was: {request.message}"
            
        # Apply filters - still applying filters to show they work
        is_toxic, toxic_reason = toxicity_filter.check(llm_response)
        has_leakage, leakage_reason = data_leakage_filter.check(llm_response)
        
        if is_toxic or has_leakage:
            filter_reason = toxic_reason or leakage_reason
            safe_response = "I apologize, but I cannot provide a response to that query as it may contain sensitive information or violate content policies."
            
            logger.warning(f"Response filtered: {filter_reason}")
            
            return QueryResponse(
                response=safe_response,
                model_used=model,
                filtered=True,
                filter_reason=filter_reason
            )
        
        return QueryResponse(
            response=llm_response,
            model_used=model,
            filtered=False
        )
            
    except httpx.RequestError as e:
        logger.error(f"Error calling model service: {str(e)}")
        raise HTTPException(status_code=503, detail=f"Model service unavailable: {str(e)}")

@app.get("/health")
async def health_check():
    return {"status": "healthy"}
