# ========================================
# ARCHIVED: LLM Service Local Inference Requirements
# ========================================
# This file is ARCHIVED and not used in production builds
# STING now uses external-ai service for all LLM inference
# This service is not built or deployed by default
# ========================================

# Core dependencies
requests>=2.28.0
# huggingface_hub>=0.16.0  # Only needed for local model downloads
tqdm>=4.64.0

# Model handling (ARCHIVED - CPU only to avoid CUDA downloads)
# transformers>=4.30.0
# torch>=2.0.0 --index-url https://download.pytorch.org/whl/cpu
# accelerate>=0.20.0

# API and service components
fastapi>=0.95.0
uvicorn>=0.22.0
pydantic>=2.0.0

# Utilities
numpy>=1.23.0
# scipy>=1.10.0  # Not needed for proxy service
