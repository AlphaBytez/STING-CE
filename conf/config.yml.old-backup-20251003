# System Configuration
system:
  domain: localhost  # Default domain for STING
  protocol: https
  ports:
    frontend: 8443
    api: 5050
    kratos: 4433

# Edition Configuration (CE vs Enterprise)
edition:
  type: ce  # Options: 'ce' (Community Edition) or 'enterprise'
  hide_enterprise_ui: true  # Hide enterprise features in UI
  enterprise_endpoints_enabled: false  # Disable enterprise API endpoints
  enterprise_features:
    marketplace: false
    teams: false
    swarm_orchestration: false
    advanced_pii_compliance: false
    nectar_bot_manager: false
  placeholder_text: "This feature is available in STING Enterprise Edition"
  placeholder_link: "https://github.com/sting-ce/enterprise"

# Core Application Settings
application:
  env: development
  debug: true
  host: localhost
  port: 5050
  install_dir: /Volumes/EXT-SSD/sting-ce
  # Directory to store LLM models. Leave blank to use ${INSTALL_DIR}/models
  models_dir: /Volumes/EXT-SSD/sting-ce/models
  ssl:
    enabled: true
    cert_dir: "${INSTALL_DIR}/certs"
    domain: "${DOMAIN_NAME:-localhost}"
    email: "${CERTBOT_EMAIL:-your-email@example.com}"
  # Session configuration for WSL2/VM compatibility
  session:
    # Set to null/None to auto-detect (better for WSL2), or specify domain
    cookie_domain: null
    # Cookie settings can be overridden per environment
    # Options: 'auto' (detect environment), 'strict' (Lax), 'permissive' (None)
    cookie_mode: auto

# Database Configuration
# Separate databases for better security and separation of concerns
databases:
  # Kratos authentication database
  kratos:
    host: db
    port: 5432
    name: kratos
    user: kratos_user
    password: kratos_secure_password_change_me  # Should be overridden by environment variable
    connection_timeout: 30
    max_connections: 50
  # Application database
  app:
    host: db
    port: 5432
    name: sting_app
    user: app_user
    password: app_secure_password_change_me  # Should be overridden by environment variable
    connection_timeout: 30
    max_connections: 100
  # Messaging database (optional separate database)
  messaging:
    host: db
    port: 5432
    name: sting_messaging
    user: app_user
    password: app_secure_password_change_me  # Should be overridden by environment variable
    connection_timeout: 30
    max_connections: 50

# Security Settings
security:
  # Authentication session configuration
  authentication:
    # AAL2 (Two-Factor Authentication) session timeout
    # Controls how long users can access sensitive operations after 2FA
    # Industry standards: 15m (banking), 1h (healthcare), 4h (enterprise), 8h (workday)
    aal2_session_timeout: "8h"
    
  supertokens:
      enabled: true
      dashboard_enabled: true
      cors_origins:
        - "https://localhost:8443"
      features:
        passkeys: true
        email_password: true
        session:
          jwt_enabled: true
          refresh_token_validity: 2592000  # 30 days
          access_token_validity: 3600      # 1 hour
      # Vault-related settings
      vault_path: "sting/supertokens"
      webauthn:
        enabled: true
        rp_id: "${HOSTNAME:-localhost}"
        rp_name: "STING"
        rp_origins:
          - "https://localhost:8443"
          - "https://${HOSTNAME:-your-production-domain.com}"

# Frontend Configuration
frontend:
  react:
    port: 8443
    api_url: "https://localhost:5050"
  
  # Development-specific settings
  development:
    hot_reload: true
    debug_tools: true

# Email Configuration
email_service:
  provider: "smtp"  # or aws_ses, sendgrid, etc.
  smtp:
    host: "${SMTP_HOST:-smtp.gmail.com}"
    port: "${SMTP_PORT:-587}"
    username: "${SMTP_USERNAME}"
    password: "${SMTP_PASSWORD}"
    from_address: "${SMTP_FROM:-no-reply@yourdomain.com}"

# Docker Configuration
docker:
  network: sting_local
  registry:
    host: ""  # Leave empty for automatic detection
    port: "5000"

# Backup Configuration
backup:
  enabled: true
  default_directory: /opt/sting-backups
  compression_level: 5
  
  # Backup retention settings
  retention:
    # Number of backups to keep (set to 1-2 for testing)
    count: 1
    # Maximum age in days (optional, 0 = disabled)
    max_age_days: 30
    # Maximum total size in GB (optional, 0 = disabled)
    max_total_size_gb: 50
    # Auto-cleanup on startup
    auto_cleanup: true
  
  # Installation backup retention (for reinstall operations)
  installation_backups:
    # Number of installation backups to keep
    retention_count: 1
    # Auto-cleanup after successful reinstall
    auto_cleanup_on_success: true
  
  exclude_patterns:
    - "*.tmp"
    - "*.log"
    - "node_modules"
    - "venv"
    - ".git"
    - "*.backup.*"  # Exclude nested backup directories

# Storage Configuration
storage:
  volumes:
    - name: postgres_data
      mount: /var/lib/postgres/data
    - name: sting_supertokens_data
      mount: /data
    - name: vault_data
      mount: /vault/file
  init_scripts:
    - source: ./conf/init_db.sql
      target: /docker-entrypoint-initdb.d/init.sql

# Monitoring and Health Checks
monitoring:
  health_checks:
    enabled: true
    interval: 30s
    timeout: 10s
    retries: 3
    start_period: 40s
  logging:
    level: INFO
    max_size: 100M
    max_files: 5
# Kratos Authentication Configuration
kratos:
  public_url: "https://kratos:4433"
  admin_url: "https://kratos:4434"
  cookie_domain: "localhost"
  # Session secret will be generated via vault or env
  # SESSION_SECRET can be set in your environment
  
  # Self-service flow settings
  selfservice:
    default_return_url: "https://localhost:8443"
    login:
      ui_url: "https://localhost:8443/login"
      lifespan: "1h"
    registration:
      ui_url: "https://localhost:8443/register"
      lifespan: "1h"
  
  # Authentication methods
  methods:
    password:
      enabled: true
    webauthn:
      enabled: true
      rp_id: "localhost"
      display_name: "STING Authentication"
      origin: "https://localhost:8443"
    oidc:
      enabled: false
      providers:
        - id: "google"
          provider: "google"
          client_id: ""
          client_secret: ""
          scopes:
            - "openid"
            - "profile"
            - "email"
        - id: "github"
          provider: "github"
          client_id: ""
          client_secret: ""
          scopes:
            - "openid"
            - "profile"
            - "email"
  
  # Email configuration
  courier:
    smtp:
      connection_uri: "smtp://test:test@mailpit:1025/?skip_ssl_verify=true"
llm_service:
  enabled: true
  gateway:
    port: 8080
    log_level: INFO
    timeout: 30
    max_retries: 3
  default_model: phi3  # Use Phi-3 Medium for enterprise-grade responses
  # Hardware acceleration settings (Apple Silicon optimized)
  hardware:
    device: "mps"  # Force Metal Performance Shaders for Apple Silicon
    precision: "fp16"  # Apple Silicon excels at fp16
    max_memory: "auto"  # Let PyTorch manage memory automatically
  
  # Performance profiles for different deployment scenarios
  performance:
    # Apple Silicon balanced profile - optimized for Mac performance
    profile: "vm_optimized"  # Good base, overridden below for Apple Silicon
    
    # VM/Virtual Appliance optimized settings
    vm_optimized:
      quantization: "int8"  # Reduce model size by ~75%
      cpu_threads: "auto"  # Use all available CPU cores
      batch_size: 1  # Optimize for single requests
      max_tokens: 512  # Limit response length for speed
      
    # GPU accelerated settings  
    gpu_accelerated:
      quantization: "none"  # Full precision for best quality
      batch_size: 4  # Process multiple requests
      max_tokens: 2048  # Allow longer responses
      
    # Cloud deployment settings
    cloud:
      quantization: "none"
      batch_size: 8
      max_tokens: 4096
  models:
    # Small, fast models for quick responses
    tinyllama:
      enabled: true
      endpoint: "http://tinyllama-service:8000/generate"
      path: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"  # Use HuggingFace model name
      max_tokens: 512
      temperature: 0.7
      size: "1.1B"  # ~2.2GB
    phi2:
      enabled: true
      endpoint: "http://phi2-service:8000/generate" 
      path: "microsoft/phi-2"  # Use HuggingFace model name
      max_tokens: 1024
      temperature: 0.7
      size: "2.7B"  # ~2.7GB
    dialogpt:
      enabled: false  # Disable for simplicity
      endpoint: "http://dialogpt-service:8000/generate"
      path: "microsoft/DialoGPT-medium"
      max_tokens: 256
      temperature: 0.8
      size: "345M"  # ~345MB
    # Llama 3 8B - Best for agent capabilities
    llama3:
      enabled: true
      endpoint: "http://llm-service:8000/generate"
      path: "microsoft/Phi-3-mini-4k-instruct"  # Use HuggingFace model name
      max_tokens: 2048
      temperature: 0.7
      size: "3.8B"  # Using phi-3-mini-4k as llama3 replacement
      precision: "fp16"  # Use fp16 for MPS efficiency
      device: "mps"      # Use Metal Performance Shaders
    # Phi-3 Medium - Enterprise-grade model from Microsoft (Apple Silicon optimized)
    phi3:
      enabled: true
      endpoint: "http://phi3-service:8000/generate"
      path: "microsoft/Phi-3-mini-4k-instruct"  # Use HuggingFace model name
      max_tokens: 2048
      temperature: 0.7
      size: "14B"  # Larger but excellent for enterprise use
      precision: "fp16"  # Optimal for Apple Silicon MPS
      device: "mps"       # Force Metal Performance Shaders
    # Keep one larger model option (disabled by default)
    phi3_mini:
      enabled: false
      endpoint: "http://phi3-service:8000/generate"
      path: "/app/models/phi-3-mini-4k"
      max_tokens: 2048
      temperature: 0.7
      size: "3.8B"  # ~7GB but much faster than Llama-3
    # DeepSeek R1 - Reasoning model with internal dialogue filtering
    deepseek-1.5b:
      enabled: true
      endpoint: "http://deepseek-service:8000/generate"
      path: "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"  # Use HuggingFace model name
      max_tokens: 2048
      temperature: 0.7
      size: "1.5B"  # ~3GB
      precision: "fp16"
      device: "mps"      # Use Metal Performance Shaders
      custom_features:
        hide_internal_dialogue: true  # Filter out <think> tags
  
  filtering:
    toxicity:
      enabled: true
      threshold: 0.7
      model: "detoxify"
    data_leakage:
      enabled: true
      sensitive_patterns:
        - "api_key"
        - "password"
        - "secret"
        - "token"
        - "internal"
    content_policy:
      block_categories:
        - "hate"
        - "harassment"
        - "self-harm"
        - "sexual"
        - "violence"
  
  routing:
    default_threshold: 0.6
    default_model: "phi3"
  
  # Model lifecycle management (Apple Silicon optimized)
  model_lifecycle:
    # Load models at startup for faster first responses
    lazy_loading: false
    
    # Keep models in memory longer for speed (2 hours)
    idle_timeout: 600  # 10 hours for phi3 stability
    
    # Keep 2 models loaded simultaneously (phi3 + backup)
    max_loaded_models: 1
    
    # Preload models during startup for immediate availability
    preload_on_startup: true
    
    # Balanced mode - not full development but optimized for speed
    development_mode: false
    
    # Model priority for eviction (lower = higher priority to keep)
    model_priorities:
      tinyllama: 1    # Keep small models loaded
      phi3: 2         # Enterprise chat model - high priority
      phi2: 3
      deepseek-1.5b: 4
      zephyr: 5
      llama3: 6       # Evict large models first
  
  # Task-based model routing
  task_routing:
    enabled: true
    
    # Define task types and their preferred models
    task_models:
      # General conversation - use small efficient model
      chat:
        primary: "phi3"
        fallback: "phi2"
        
      # Agent/tool use - use more capable models
      agent:
        primary: "deepseek-1.5b"  # DeepSeek is good at reasoning
        fallback: "llama3"
        
      # Analysis tasks - use larger, more capable models
      analysis:
        primary: "llama3"
        fallback: "zephyr"
        
      # Code generation - use specialized models
      coding:
        primary: "deepseek-1.5b"  # DeepSeek excels at code
        fallback: "llama3"
        
      # Summarization - use efficient models
      summarization:
        primary: "phi2"
        fallback: "tinyllama"
    
    # Task detection keywords/patterns
    task_detection:
      agent:
        keywords: ["search", "find", "analyze", "calculate", "compute", "tool", "function"]
        patterns: ["can you.*for me", "please.*and.*then", "first.*then.*finally"]
        
      analysis:
        keywords: ["analyze", "compare", "evaluate", "assess", "review", "investigate"]
        patterns: ["what.*think about", "how.*compare", "pros and cons"]
        
      coding:
        keywords: ["code", "program", "script", "function", "class", "debug", "implement"]
        patterns: ["write.*code", "create.*function", "fix.*bug", "implement.*algorithm"]
        
      summarization:
        keywords: ["summarize", "summary", "brief", "overview", "tldr", "main points"]
        patterns: ["sum.*up", "give.*overview", "main.*points"]


# Honey Reserve Configration (User Storage Management)
honey_reserve:
  enabled: true
  default_quota: 1073741824  # 1GB per user in bytes
  
  # Storage paths
  storage_paths:
    primary: "${INSTALL_DIR}/honey-reserve"
    temp_uploads: "${INSTALL_DIR}/temp-uploads"
    archives: "${INSTALL_DIR}/archives"
  
  # File upload settings
  file_upload:
    max_file_size: 104857600  # 100MB
    allowed_extensions: ["pdf", "docx", "doc", "txt", "md", "json", "csv", "html", "htm", "png", "jpg", "jpeg"]
    temp_retention_hours: 48
    rate_limit_per_minute: 10
    rate_limit_per_hour: 100
  
  # Storage lifecycle
  lifecycle:
    # Temporary files auto-cleanup
    temp_cleanup_enabled: true
    temp_cleanup_schedule: "0 */2 * * *"  # Every 2 hours
    
    # Transition timings (in days)
    active_to_standard_days: 2
    standard_to_archive_days: 30
    archive_to_deletion_days: 365
    
    # Auto-archival settings
    auto_archive_enabled: true
    archive_files_larger_than: 52428800  # 50MB
  
  # Quotas and limits
  quotas:
    warning_threshold_percent: 90
    critical_threshold_percent: 95
    auto_cleanup_at_percent: 100
    
    # User-specific quotas (override default)
    custom_quotas:
      # admin@sting.local: 5368709120  # 5GB for admin
  
  # Maintenance settings
  maintenance:
    # Daily maintenance window
    window_start: "02:00"
    window_duration_hours: 2
    
    # Integrity checks
    verify_checksums: true
    check_orphaned_files: true
    optimize_storage: true
  
  # Backup integration
  backup:
    include_in_backups: true
    backup_priority: "high"
    compress_before_backup: true
    encryption_enabled: true
  
  # Security
  security:
    encrypt_at_rest: true
    encryption_algorithm: "AES-256-GCM"
    key_derivation: "HKDF-SHA256"
    audit_all_access: true

# Public Bee Service - AI-as-a-Service Chat API
public_bee:
  # Core service settings
  enabled: false  # Disabled by default, admin must enable
  host: 0.0.0.0
  port: 8092
  debug: false
  
  # Demo configuration
  demo_mode: true
  create_demo_bot: true
  demo_bot:
    name: "sting-assistant"
    display_name: "STING Assistant"
    description: "AI assistant trained on STING platform documentation"
    honey_jar_ids: ["sting-platform-docs"]
    rate_limit: 50
    
  # Default bot settings
  defaults:
    rate_limit: 100  # requests per hour
    max_concurrent: 5  # concurrent sessions per API key
    system_prompt: "You are a helpful AI assistant with access to knowledge bases."
    response_max_tokens: 500
    temperature: 0.7
    
  # Security settings
  security:
    require_api_key: true
    enable_rate_limiting: true
    enable_ip_whitelisting: false
    log_all_interactions: true
    pii_filtering: true
    content_filter_level: "moderate"  # strict, moderate, minimal
    
    # API key settings
    api_key_expiration_days: 365
    max_api_keys_per_bot: 10
    
  # Integration settings
  integrations:
    # AI service endpoints
    external_ai_url: "http://external-ai:8091"
    chatbot_url: "http://chatbot:8888"
    knowledge_service_url: "http://knowledge:8090"
    
    # Response enhancement
    use_knowledge_context: true
    max_knowledge_results: 5
    knowledge_relevance_threshold: 0.7

# Nectar Bots - AI-as-a-Service Bot Management
nectar_bots:
  # Core service settings
  enabled: true
  max_bots_per_organization: 10
  max_bots_per_user: 5
  
  # Default bot configuration
  defaults:
    system_prompt: "You are a helpful AI assistant with access to knowledge bases."
    max_conversation_length: 20
    confidence_threshold: 0.7
    rate_limit_per_hour: 100
    rate_limit_per_day: 1000
    handoff_enabled: true
    handoff_keywords: ["help", "human", "support", "escalate", "agent"]
    handoff_confidence_threshold: 0.6
    
  # Handoff system configuration
  handoff:
    # CE Edition: Internal handoff to admin users
    mode: "ce_internal"  # Options: ce_internal, enterprise_external
    
    # CE Internal handoff settings
    ce_internal:
      notification_methods: ["in_app", "email"]
      target_roles: ["admin"]
      urgency_mapping:
        low: "info"
        medium: "warning"
        high: "urgent"
        critical: "critical"
      
      # Automatic handoff triggers
      triggers:
        confidence_threshold: 0.6
        keywords: ["help", "human", "support", "escalate", "speak to person"]
        max_conversation_length: 25
        repeated_failures: 3
        time_based_escalation_minutes: 30
        
      # Response time tracking
      sla_targets:
        response_time_minutes: 15
        resolution_time_hours: 4
        escalation_time_hours: 24
    
    # Enterprise External handoff settings (future)
    enterprise_external:
      enabled: false
      webhooks:
        slack:
          enabled: false
          webhook_url: ""
          channels: ["#support"]
          mention_groups: ["@support-team"]
        
        teams:
          enabled: false
          webhook_url: ""
          channel: "Customer Support"
          
        zendesk:
          enabled: false
          api_endpoint: ""
          api_key: ""
          ticket_priority: "normal"
          auto_assign: true
          
        pagerduty:
          enabled: false
          integration_key: ""
          severity: "warning"
          auto_acknowledge: false
  
  # Analytics and monitoring
  analytics:
    enabled: true
    retention_days: 90
    track_metrics:
      - conversation_count
      - message_count
      - handoff_rate
      - resolution_time
      - user_satisfaction
      - confidence_scores
    
    # Performance thresholds
    performance_thresholds:
      min_confidence: 0.7
      max_handoff_rate: 0.15  # 15% max handoff rate
      target_resolution_time_minutes: 240  # 4 hours
      
  # Security settings
  security:
    api_key_rotation_days: 90
    rate_limiting:
      enabled: true
      burst_limit: 10
      sustained_limit: 100
      window_minutes: 60
      
    content_filtering:
      enabled: true
      pii_detection: true
      toxicity_filter: true
      safety_level: "moderate"  # strict, moderate, permissive
      
    audit_logging:
      log_all_interactions: true
      log_pii_detections: true
      log_handoffs: true
      retention_days: 365
      
  # Integration settings
  integrations:
    messaging_service:
      enabled: true
      notification_service_url: "http://messaging:8080"
      
    knowledge_service:
      enabled: true
      honey_jar_access_control: true
      max_honey_jars_per_bot: 5
      
    external_ai:
      enabled: true
      primary_endpoint: "http://external-ai:8091"
      fallback_endpoint: "http://chatbot:8888"
      
  # Database settings
  database:
    table_prefix: "nectar_bot_"
    enable_indexing: true
    cleanup_interval_days: 30
    archive_conversations_days: 90

# Knowledge Base Configuration (Honey Pot System)
knowledge_base:
  enabled: true
  service_port: 8090
  chroma_url: "http://chroma:8000"
  
  # Vector database settings
  vector_db:
    collection_name: "sting_knowledge"
    embedding_model: "all-MiniLM-L6-v2"  # Fast, efficient embeddings
    chunk_size: 500
    chunk_overlap: 50
    
  # Document processing
  document_processing:
    supported_formats: ["pdf", "docx", "txt", "md", "json", "html"]
    max_file_size_mb: 50
    ocr_enabled: false  # Enable for scanned documents
    
  # Default Honey Jars
  default_honey_jars:
    - name: "STING Documentation"
      description: "Core platform documentation and guides"
      type: "system"
      auto_populate: true
    - name: "Business Knowledge"
      description: "Your organization's knowledge base"
      type: "private"
      auto_populate: false
      
  # Bee Integration
  bee_integration:
    enabled: true
    auto_query: true  # Automatically search knowledge base for relevant context
    context_injection: true  # Inject relevant knowledge into Bee's responses
    confidence_threshold: 0.75
    
  # Security
  security:
    encryption_at_rest: true
    access_control: true
    audit_logging: true
      
# Chatbot Configuration
chatbot:
  enabled: true
  name: "Bee"
  model: "phi3"  # Default model to use - enterprise-grade with better reasoning
  context_window: 8  # Reduced for memory efficiency\n  \n  # Memory optimization settings\n  memory_optimization:\n    enable_phi3_optimizations: true\n    max_context_length: 4096\n    model_cleanup_interval: 600\n    torch_memory_fraction: 0.75
  default_system_prompt: "You are Bee, a helpful and friendly assistant for the STING platform. Answer user questions clearly and accurately. If you don't know the answer, be honest about it."
  
  # Knowledge base integration - Core feature for business customization
  knowledge_integration:
    enabled: true
    auto_enhance_responses: true
    relevance_threshold: 0.7
    max_context_documents: 5
    business_context_priority: high
  
  # Performance settings for chatbot (Apple Silicon optimized)
  performance:
    # Enable response caching for faster repeated queries
    enable_response_cache: true
    cache_ttl: 300  # Cache responses for 5 minutes
    
    # Concurrent request handling
    max_concurrent_requests: 5
    request_timeout: 60  # seconds
    
    # Response streaming for better perceived performance
    enable_streaming: true
    stream_chunk_size: 64
  
  tools:
    enabled: true
    allow_custom: true
    allowed_tools:
      - search
      - summarize
      - analyze
  
  security:
    require_authentication: true
    log_conversations: true
    content_filter_level: "strict"  # strict, moderate, minimal
  
  # Conversation management settings
  conversation:
    # Token management
    max_tokens: 4096              # Maximum tokens per conversation context
    max_messages: 50              # Maximum messages to keep in memory
    token_buffer_percent: 20      # Reserve 20% of max_tokens for response generation
    
    # Persistence
    persistence_enabled: true     # Enable database persistence for conversations
    session_timeout_hours: 24     # Hours before a session expires
    archive_after_days: 30        # Days before conversations are archived
    cleanup_interval_hours: 1     # How often to run cleanup tasks
    
    # Summarization
    summarization_enabled: true   # Enable automatic summarization of old messages
    summarize_after_messages: 20  # Summarize after this many messages
    summary_max_tokens: 200       # Maximum tokens for summary
    summary_model: "phi3:mini"  # Model to use for summarization
    
    # Pruning strategy
    pruning_strategy: "sliding_window"  # Options: sliding_window, importance_based
    keep_system_messages: true    # Always keep system messages
    keep_recent_messages: 10      # Always keep this many recent messages

# Nectar Worker - Lightweight AI service for Nectar Bots
nectar_worker:
  enabled: true

  # Ollama Configuration
  ollama:
    url: "http://ollama:11434"
    default_model: "llama3.2:latest"  # Lightweight model for Nectar Bots
    keep_alive: "30m"  # Keep model in memory for 30 minutes

  # Nectar Bot Limits (prevent abuse)
  limits:
    max_honey_jars_per_bot: 3      # Maximum Honey Jars per bot
    max_context_tokens: 2000        # Maximum context tokens from Honey Jars
    max_concurrent_requests: 10     # Max concurrent chat requests
    request_timeout: 60             # Request timeout in seconds

  # Performance & Caching
  performance:
    honey_jar_cache_size: 100       # Number of Honey Jar searches to cache
    honey_jar_cache_ttl: 300        # Cache TTL in seconds (5 minutes)
    bot_config_cache_ttl: 300       # Bot config cache TTL in seconds

  # Security
  security:
    require_api_key: true           # Require API key for internal calls
    validate_bot_access: true       # Validate bot access permissions

  # Logging
  logging:
    level: "INFO"
    log_conversations: true         # Log conversations for analytics

# Beeacon Monitoring Stack & HiveMind AI Observability Configuration
observability:
  enabled: true  # Enable observability services (Grafana, Loki, Promtail)
  
  # Grafana Dashboard Configuration
  grafana:
    enabled: true
    port: 3000
    admin_user: "admin"
    # Admin password will be auto-generated and stored in Vault
    # Dashboard provisioning
    dashboards:
      auto_provision: true
      default_dashboards:
        - "sting-system-overview"
        - "authentication-audit"  
        - "knowledge-service-metrics"
        - "pii-compliance-dashboard"
        - "bee-chat-monitoring"
    
    # Data source configuration
    datasources:
      loki:
        enabled: true
        url: "http://loki:3100"
      prometheus:
        enabled: false  # Future enhancement
        url: "http://prometheus:9090"
  
  # Loki Log Aggregation
  loki:
    enabled: true
    port: 3100
    retention_period: "168h"  # 7 days
    max_line_size: "256KB"
    
    # Storage configuration
    storage:
      type: "filesystem"
      path: "${INSTALL_DIR}/loki-data"
      
    # Compaction and cleanup
    compaction:
      enabled: true
      interval: "24h"
    
    # Limits
    limits:
      max_query_series: 500
      max_query_parallelism: 32
  
  # Promtail Log Shipping
  promtail:
    enabled: true
    port: 9080
    
    # Log sources to monitor
    log_sources:
      - name: "sting-app"
        path: "/var/log/app/*.log"
        labels:
          service: "sting-app"
          environment: "production"
      - name: "docker-logs"
        path: "/var/lib/docker/containers/*/*-json.log"
        labels:
          source: "docker"
    
    # Pollen Filter integration (log sanitization)
    pollen_filter:
      enabled: true
      sanitize_logs: true
      vault_references: true
      audit_trail: true
      
      # PII patterns to sanitize
      patterns:
        pii: true
        secrets: true 
        database_credentials: true
        auth_tokens: true
        
      # Sanitization actions
      actions:
        replace_with_hash: true
        store_in_vault: true
        audit_log: true

# Headscale Support Tunnels - Self-hosted secure access for support
headscale:
  enabled: true  # Enable Headscale support tunnels
  
  # Server configuration
  server:
    port: 8070  # Avoiding 8080 conflict
    metrics_port: 9090
    base_domain: "support.sting.local"
    listen_addr: "0.0.0.0:8070"
    
  # Database configuration
  database:
    type: "sqlite"
    path: "/var/lib/headscale/db.sqlite"
    
  # Security settings
  security:
    ephemeral_node_timeout: "30m"  # Support sessions auto-expire
    randomize_client_port: true
    enable_routing: false  # Don't route customer networks
    
  # Support session configuration
  support_sessions:
    # Community Edition: Bundle downloads only (no live tunnels)
    community:
      bundle_download_duration: "48h"  # Realistic timeframe for community help
      secure_link_enabled: true
      live_tunnel_enabled: false
      
    # Professional Edition: Short live tunnels  
    professional:
      tunnel_duration: "4h"    # Reasonable for professional support
      bundle_download_duration: "7d"
      live_tunnel_enabled: true
      
    # Enterprise Edition: Extended live access
    enterprise:
      tunnel_duration: "24h"   # Extended for complex issues
      bundle_download_duration: "30d"
      live_tunnel_enabled: true
      priority_response: "15m"
      
    max_concurrent: 10        # Max simultaneous support sessions
    
  # DNS and networking
  dns:
    magic_dns: true
    base_domain: "support.sting.local"
    nameservers:
      - "1.1.1.1"
      - "1.0.0.1"
      
  # IP ranges for support network
  ip_ranges:
    ipv4: "100.64.0.0/10"
    ipv6: "fd7a:115c:a1e0::/48"
    
  # DERP relay configuration
  derp:
    enabled: false  # Use public Tailscale DERP servers
    auto_update: true
    update_frequency: "24h"
    
  # Logging and monitoring
  logging:
    level: "info"
    file: "/var/log/headscale/headscale.log"
    
  # Policy file location
  policy_file: "/etc/headscale/policy.hujson"

# HiveMind AI Observability (Future Enhancement)
hivemind:
  enabled: false  # Will be enabled when HiveMind is released
  
  # AI-powered observability features
  features:
    anomaly_detection: true
    predictive_analytics: true
    intelligent_automation: true
    natural_language_queries: true
    
  # LLM Integration for AI analysis
  llm_integration:
    local_models:
      enabled: true
      ollama_endpoint: "http://ollama:11434"
      default_model: "llama3.2"
    
    external_apis:
      enabled: false  # Enable for enhanced capabilities
      openai_key: ""  # Set via Vault
      anthropic_key: ""  # Set via Vault
      
  # AI Analysis Configuration
  analysis:
    log_analysis_interval: "1h"
    anomaly_threshold: 0.8
    prediction_window: "24h"
    auto_correlation: true
